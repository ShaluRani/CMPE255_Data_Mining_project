{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(101)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(101)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import keras\n",
    "#from keras import backend as K\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.listdir('data/reshaped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: 'base_dir'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-254-939522bf237b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create a new directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbase_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'base_dir'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# train_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: 'base_dir'"
     ]
    }
   ],
   "source": [
    "# Create a new directory\n",
    "base_dir = 'base_dir'\n",
    "os.mkdir(base_dir)\n",
    "\n",
    "# train_dir\n",
    "train_dir = os.path.join(base_dir, 'train_dir')\n",
    "os.mkdir(train_dir)\n",
    "\n",
    "# val_dir\n",
    "val_dir = os.path.join(base_dir, 'val_dir')\n",
    "os.mkdir(val_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: 'base_dir/train_dir/nv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-255-9e73492012bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# create new folders inside train_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'nv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: 'base_dir/train_dir/nv'"
     ]
    }
   ],
   "source": [
    "# create new folders inside train_dir\n",
    "nv = os.path.join(train_dir, 'nv')\n",
    "os.mkdir(nv)\n",
    "mel = os.path.join(train_dir, 'mel')\n",
    "os.mkdir(mel)\n",
    "bkl = os.path.join(train_dir, 'bkl')\n",
    "os.mkdir(bkl)\n",
    "bcc = os.path.join(train_dir, 'bcc')\n",
    "os.mkdir(bcc)\n",
    "akiec = os.path.join(train_dir, 'akiec')\n",
    "os.mkdir(akiec)\n",
    "vasc = os.path.join(train_dir, 'vasc')\n",
    "os.mkdir(vasc)\n",
    "df = os.path.join(train_dir, 'df')\n",
    "os.mkdir(df)\n",
    "\n",
    "# create new folders inside val_dir\n",
    "nv = os.path.join(val_dir, 'nv')\n",
    "os.mkdir(nv)\n",
    "mel = os.path.join(val_dir, 'mel')\n",
    "os.mkdir(mel)\n",
    "bkl = os.path.join(val_dir, 'bkl')\n",
    "os.mkdir(bkl)\n",
    "bcc = os.path.join(val_dir, 'bcc')\n",
    "os.mkdir(bcc)\n",
    "akiec = os.path.join(val_dir, 'akiec')\n",
    "os.mkdir(akiec)\n",
    "vasc = os.path.join(val_dir, 'vasc')\n",
    "os.mkdir(vasc)\n",
    "df = os.path.join(val_dir, 'df')\n",
    "os.mkdir(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lesion_id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>dx</th>\n",
       "      <th>dx_type</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>localization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HAM_0000118</td>\n",
       "      <td>ISIC_0027419</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HAM_0000118</td>\n",
       "      <td>ISIC_0025030</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HAM_0002730</td>\n",
       "      <td>ISIC_0026769</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HAM_0002730</td>\n",
       "      <td>ISIC_0025661</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HAM_0001466</td>\n",
       "      <td>ISIC_0031633</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>75.0</td>\n",
       "      <td>male</td>\n",
       "      <td>ear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     lesion_id      image_id   dx dx_type   age   sex localization\n",
       "0  HAM_0000118  ISIC_0027419  bkl   histo  80.0  male        scalp\n",
       "1  HAM_0000118  ISIC_0025030  bkl   histo  80.0  male        scalp\n",
       "2  HAM_0002730  ISIC_0026769  bkl   histo  80.0  male        scalp\n",
       "3  HAM_0002730  ISIC_0025661  bkl   histo  80.0  male        scalp\n",
       "4  HAM_0001466  ISIC_0031633  bkl   histo  75.0  male          ear"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data = pd.read_csv('data/HAM10000_metadata.csv')\n",
    "\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lesion_id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>dx</th>\n",
       "      <th>dx_type</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>localization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HAM_0000001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HAM_0000003</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HAM_0000004</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HAM_0000007</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HAM_0000008</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     lesion_id  image_id  dx  dx_type  age  sex  localization\n",
       "0  HAM_0000001         1   1        1    1    1             1\n",
       "1  HAM_0000003         1   1        1    1    1             1\n",
       "2  HAM_0000004         1   1        1    1    1             1\n",
       "3  HAM_0000007         1   1        1    1    1             1\n",
       "4  HAM_0000008         1   1        1    1    1             1"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this will tell us how many images are associated with each lesion_id\n",
    "df = df_data.groupby('lesion_id').count()\n",
    "\n",
    "# now we filter out lesion_id's that have only one image associated with it\n",
    "df = df[df['image_id'] == 1]\n",
    "\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lesion_id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>dx</th>\n",
       "      <th>dx_type</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>localization</th>\n",
       "      <th>duplicates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HAM_0000118</td>\n",
       "      <td>ISIC_0027419</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "      <td>has_duplicates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HAM_0000118</td>\n",
       "      <td>ISIC_0025030</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "      <td>has_duplicates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HAM_0002730</td>\n",
       "      <td>ISIC_0026769</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "      <td>has_duplicates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HAM_0002730</td>\n",
       "      <td>ISIC_0025661</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "      <td>has_duplicates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HAM_0001466</td>\n",
       "      <td>ISIC_0031633</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>75.0</td>\n",
       "      <td>male</td>\n",
       "      <td>ear</td>\n",
       "      <td>has_duplicates</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     lesion_id      image_id   dx dx_type   age   sex localization  \\\n",
       "0  HAM_0000118  ISIC_0027419  bkl   histo  80.0  male        scalp   \n",
       "1  HAM_0000118  ISIC_0025030  bkl   histo  80.0  male        scalp   \n",
       "2  HAM_0002730  ISIC_0026769  bkl   histo  80.0  male        scalp   \n",
       "3  HAM_0002730  ISIC_0025661  bkl   histo  80.0  male        scalp   \n",
       "4  HAM_0001466  ISIC_0031633  bkl   histo  75.0  male          ear   \n",
       "\n",
       "       duplicates  \n",
       "0  has_duplicates  \n",
       "1  has_duplicates  \n",
       "2  has_duplicates  \n",
       "3  has_duplicates  \n",
       "4  has_duplicates  "
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here we identify lesion_id's that have duplicate images and those that have only\n",
    "# one image.\n",
    "\n",
    "def identify_duplicates(x):\n",
    "    \n",
    "    unique_list = list(df['lesion_id'])\n",
    "    \n",
    "    if x in unique_list:\n",
    "        return 'no_duplicates'\n",
    "    else:\n",
    "        return 'has_duplicates'\n",
    "    \n",
    "# create a new colum that is a copy of the lesion_id column\n",
    "df_data['duplicates'] = df_data['lesion_id']\n",
    "# apply the function to this new column\n",
    "df_data['duplicates'] = df_data['duplicates'].apply(identify_duplicates)\n",
    "\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no_duplicates     5514\n",
       "has_duplicates    4501\n",
       "Name: duplicates, dtype: int64"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data['duplicates'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5514, 8)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we filter out images that don't have duplicates\n",
    "df = df_data[df_data['duplicates'] == 'no_duplicates']\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop_duplicates('lesion_id') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5514\n"
     ]
    }
   ],
   "source": [
    "df['train_or_val'] = df['image_id']\n",
    "print(df['dx'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4962, 9)\n",
      "(552, 9)\n"
     ]
    }
   ],
   "source": [
    "# now we create a val set using df because we are sure that none of these images\n",
    "# have augmented duplicates in the train set\n",
    "y = df['dx']\n",
    "\n",
    "df_train, df_val = train_test_split(df, test_size=0.1, random_state=101, stratify=y)\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nv       442\n",
       "bkl       44\n",
       "mel       23\n",
       "bcc       18\n",
       "akiec     15\n",
       "vasc       6\n",
       "df         4\n",
       "Name: dx, dtype: int64"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val['dx'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nv       3973\n",
       "bkl       396\n",
       "mel       207\n",
       "bcc       157\n",
       "akiec     136\n",
       "vasc       58\n",
       "df         35\n",
       "Name: dx, dtype: int64"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['dx'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4962\n",
      "552\n"
     ]
    }
   ],
   "source": [
    "# This set will be df_data excluding all rows that are in the val set\n",
    "\n",
    "# This function identifies if an image is part of the train\n",
    "# or val set.\n",
    "def identify_val_rows(x):\n",
    "    # create a list of all the lesion_id's in the val set\n",
    "    val_list = list(df_val['image_id'])\n",
    "    \n",
    "    if str(x) in val_list:\n",
    "        return 'val'\n",
    "    else:\n",
    "        return 'train'\n",
    "\n",
    "# identify train and val rows\n",
    "\n",
    "# create a new colum that is a copy of the image_id column\n",
    "df['train_or_val'] = df['image_id']\n",
    "# apply the function to this new column\n",
    "df['train_or_val'] = df['train_or_val'].apply(identify_val_rows)\n",
    "   \n",
    "# filter out train rows\n",
    "df_train = df[df['train_or_val'] == 'train']\n",
    "\n",
    "\n",
    "print(len(df_train))\n",
    "print(len(df_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lesion_id       5514\n",
      "dx              5514\n",
      "dx_type         5514\n",
      "age             5467\n",
      "sex             5514\n",
      "localization    5514\n",
      "duplicates      5514\n",
      "train_or_val    5514\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Set the image_id as the index in df_data\n",
    "df.set_index('image_id', inplace=True)\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of images\n",
    "folder_1 = os.listdir('data/reshaped')\n",
    "\n",
    "# Get a list of train and val images\n",
    "train_list = list(df_train['image_id'])\n",
    "val_list = list(df_val['image_id'])\n",
    "\n",
    "#print((train_list))\n",
    "\n",
    "\n",
    "\n",
    "# Transfer the train images\n",
    "for image in train_list:\n",
    "    \n",
    "    fname = image + '.jpg'\n",
    "    label = df.loc[image,'dx']\n",
    "    \n",
    "    if fname in folder_1:\n",
    "        # source path to image\n",
    "        src = os.path.join('data/reshaped', fname)\n",
    "        # destination path to image\n",
    "        dst = os.path.join(train_dir, label, fname)\n",
    "        # copy the image from the source to the destination\n",
    "        shutil.copyfile(src, dst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer the val images\n",
    "\n",
    "for image in val_list:\n",
    "    \n",
    "    fname = image + '.jpg'\n",
    "    label = df.loc[image,'dx']\n",
    "    \n",
    "    if fname in folder_1:\n",
    "        # source path to image\n",
    "        src = os.path.join('data/reshaped', fname)\n",
    "        # destination path to image\n",
    "        dst = os.path.join(val_dir, label, fname)\n",
    "        # copy the image from the source to the destination\n",
    "        shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.4'"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.0'"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow\n",
    "tensorflow.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in /Users/shalu/anaconda3/lib/python3.6/site-packages (5.1.0)\n",
      "/Users/shalu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##This notebook is built around using tensorflow as the backend for keras\n",
    "!pip install pillow\n",
    "!KERAS_BACKEND=tensorflow python -c \"from keras import backend\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions of our images.\n",
    "img_width, img_height = 100, 75\n",
    "\n",
    "train_data_dir = 'base_dir/train_dir'\n",
    "validation_data_dir = 'base_dir/val_dir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4962 images belonging to 7 classes.\n",
      "Found 552 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# used to rescale the pixel values from [0, 255] to [0, 1] interval\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# automagically retrieve images and their classes for train and validation sets\n",
    "train_generator = datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=16,\n",
    "        class_mode=\"categorical\")\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=32,\n",
    "        class_mode=\"categorical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_13 (Conv2D)           (None, 98, 73, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 98, 73, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 49, 36, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 47, 34, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 47, 34, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 23, 17, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 21, 15, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 21, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 10, 7, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 4480)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 64)                286784    \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 7)                 455       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 7)                 0         \n",
      "=================================================================\n",
      "Total params: 315,879\n",
      "Trainable params: 315,879\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shalu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(100, 75, ...)`\n",
      "  \n",
      "/Users/shalu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3))`\n",
      "  \n",
      "/Users/shalu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3))`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Convolution2D(32, 3, 3, input_shape=(img_width, img_height,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(32, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(7))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epoch = 50\n",
    "nb_train_samples = 4962\n",
    "nb_validation_samples = 552"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shalu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "  \n",
      "/Users/shalu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., validation_data=<keras_pre..., steps_per_epoch=310, epochs=50, validation_steps=552)`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "310/310 [==============================] - 65s 209ms/step - loss: 0.8055 - acc: 0.7962 - val_loss: 0.7534 - val_acc: 0.8009\n",
      "Epoch 2/50\n",
      "310/310 [==============================] - 64s 206ms/step - loss: 0.6671 - acc: 0.8008 - val_loss: 0.6127 - val_acc: 0.8005\n",
      "Epoch 3/50\n",
      "310/310 [==============================] - 63s 204ms/step - loss: 0.6357 - acc: 0.7998 - val_loss: 0.6218 - val_acc: 0.8007\n",
      "Epoch 4/50\n",
      "310/310 [==============================] - 63s 202ms/step - loss: 0.6241 - acc: 0.8020 - val_loss: 0.5964 - val_acc: 0.8007\n",
      "Epoch 5/50\n",
      "310/310 [==============================] - 62s 200ms/step - loss: 0.6144 - acc: 0.8052 - val_loss: 0.6316 - val_acc: 0.8165\n",
      "Epoch 6/50\n",
      "310/310 [==============================] - 62s 201ms/step - loss: 0.6105 - acc: 0.8052 - val_loss: 0.5664 - val_acc: 0.8066\n",
      "Epoch 7/50\n",
      "310/310 [==============================] - 62s 201ms/step - loss: 0.6162 - acc: 0.8060 - val_loss: 0.7585 - val_acc: 0.8062\n",
      "Epoch 8/50\n",
      "310/310 [==============================] - 65s 208ms/step - loss: 0.6246 - acc: 0.8091 - val_loss: 0.6382 - val_acc: 0.8226\n",
      "Epoch 9/50\n",
      "310/310 [==============================] - 62s 200ms/step - loss: 0.6115 - acc: 0.8081 - val_loss: 0.5562 - val_acc: 0.8147\n",
      "Epoch 10/50\n",
      "310/310 [==============================] - 61s 195ms/step - loss: 0.6004 - acc: 0.8063 - val_loss: 0.6418 - val_acc: 0.8063\n",
      "Epoch 11/50\n",
      "310/310 [==============================] - 60s 193ms/step - loss: 0.6342 - acc: 0.8060 - val_loss: 0.6302 - val_acc: 0.8133\n",
      "Epoch 12/50\n",
      "310/310 [==============================] - 60s 193ms/step - loss: 0.6360 - acc: 0.8127 - val_loss: 0.5919 - val_acc: 0.8153\n",
      "Epoch 13/50\n",
      "310/310 [==============================] - 60s 192ms/step - loss: 0.6079 - acc: 0.8062 - val_loss: 0.6080 - val_acc: 0.8182\n",
      "Epoch 14/50\n",
      "310/310 [==============================] - 63s 202ms/step - loss: 0.6306 - acc: 0.8111 - val_loss: 0.6309 - val_acc: 0.8160\n",
      "Epoch 15/50\n",
      "310/310 [==============================] - 71s 229ms/step - loss: 0.6314 - acc: 0.8038 - val_loss: 0.6663 - val_acc: 0.8134\n",
      "Epoch 16/50\n",
      "310/310 [==============================] - 71s 228ms/step - loss: 0.6216 - acc: 0.8083 - val_loss: 0.6547 - val_acc: 0.8186\n",
      "Epoch 17/50\n",
      "310/310 [==============================] - 70s 225ms/step - loss: 0.6168 - acc: 0.8079 - val_loss: 0.7387 - val_acc: 0.8209\n",
      "Epoch 18/50\n",
      "310/310 [==============================] - 69s 223ms/step - loss: 0.6420 - acc: 0.8109 - val_loss: 0.7873 - val_acc: 0.8152\n",
      "Epoch 19/50\n",
      "310/310 [==============================] - 71s 230ms/step - loss: 0.6340 - acc: 0.8067 - val_loss: 0.6233 - val_acc: 0.8207\n",
      "Epoch 20/50\n",
      "310/310 [==============================] - 72s 232ms/step - loss: 0.7380 - acc: 0.8061 - val_loss: 0.8686 - val_acc: 0.8169\n",
      "Epoch 21/50\n",
      "310/310 [==============================] - 72s 233ms/step - loss: 0.6331 - acc: 0.8101 - val_loss: 1.1233 - val_acc: 0.8116\n",
      "Epoch 22/50\n",
      "310/310 [==============================] - 72s 233ms/step - loss: 1.3122 - acc: 0.8101 - val_loss: 0.6334 - val_acc: 0.8209\n",
      "Epoch 23/50\n",
      "310/310 [==============================] - 72s 233ms/step - loss: 0.6219 - acc: 0.8131 - val_loss: 0.7077 - val_acc: 0.8167\n",
      "Epoch 24/50\n",
      "310/310 [==============================] - 67s 216ms/step - loss: 0.8192 - acc: 0.8121 - val_loss: 0.7037 - val_acc: 0.8260\n",
      "Epoch 25/50\n",
      "310/310 [==============================] - 65s 211ms/step - loss: 0.6229 - acc: 0.8125 - val_loss: 0.7984 - val_acc: 0.8260\n",
      "Epoch 26/50\n",
      "310/310 [==============================] - 67s 217ms/step - loss: 0.6379 - acc: 0.8091 - val_loss: 0.6125 - val_acc: 0.8244\n",
      "Epoch 27/50\n",
      "310/310 [==============================] - 67s 215ms/step - loss: 0.6000 - acc: 0.8095 - val_loss: 0.8013 - val_acc: 0.8258\n",
      "Epoch 28/50\n",
      "310/310 [==============================] - 66s 214ms/step - loss: 0.9872 - acc: 0.8079 - val_loss: 0.6560 - val_acc: 0.8225\n",
      "Epoch 29/50\n",
      "310/310 [==============================] - 67s 216ms/step - loss: 0.9895 - acc: 0.8063 - val_loss: 3.2217 - val_acc: 0.8001\n",
      "Epoch 30/50\n",
      "310/310 [==============================] - 68s 220ms/step - loss: 3.1619 - acc: 0.8038 - val_loss: 3.2084 - val_acc: 0.8009\n",
      "Epoch 31/50\n",
      "310/310 [==============================] - 68s 219ms/step - loss: 3.2919 - acc: 0.7958 - val_loss: 3.2172 - val_acc: 0.8004\n",
      "Epoch 32/50\n",
      "310/310 [==============================] - 68s 219ms/step - loss: 3.2236 - acc: 0.8000 - val_loss: 3.2017 - val_acc: 0.8014\n",
      "Epoch 33/50\n",
      "310/310 [==============================] - 68s 218ms/step - loss: 3.2042 - acc: 0.8012 - val_loss: 3.2170 - val_acc: 0.8004\n",
      "Epoch 34/50\n",
      "310/310 [==============================] - 68s 219ms/step - loss: 3.1912 - acc: 0.8020 - val_loss: 3.2124 - val_acc: 0.8007\n",
      "Epoch 35/50\n",
      "310/310 [==============================] - 68s 219ms/step - loss: 3.2204 - acc: 0.8002 - val_loss: 3.2122 - val_acc: 0.8007\n",
      "Epoch 36/50\n",
      "310/310 [==============================] - 70s 225ms/step - loss: 3.1457 - acc: 0.8048 - val_loss: 3.2112 - val_acc: 0.8008\n",
      "Epoch 37/50\n",
      "310/310 [==============================] - 65s 210ms/step - loss: 3.2789 - acc: 0.7966 - val_loss: 3.2143 - val_acc: 0.8006\n",
      "Epoch 38/50\n",
      "310/310 [==============================] - 70s 227ms/step - loss: 3.1457 - acc: 0.8048 - val_loss: 3.2112 - val_acc: 0.8008\n",
      "Epoch 39/50\n",
      "310/310 [==============================] - 63s 203ms/step - loss: 3.2236 - acc: 0.8000 - val_loss: 3.2103 - val_acc: 0.8008\n",
      "Epoch 40/50\n",
      "310/310 [==============================] - 65s 211ms/step - loss: 3.2204 - acc: 0.8002 - val_loss: 3.2124 - val_acc: 0.8007\n",
      "Epoch 41/50\n",
      "310/310 [==============================] - 65s 211ms/step - loss: 3.2788 - acc: 0.7966 - val_loss: 3.2103 - val_acc: 0.8008\n",
      "Epoch 42/50\n",
      "310/310 [==============================] - 63s 202ms/step - loss: 3.1164 - acc: 0.8067 - val_loss: 3.2131 - val_acc: 0.8007\n",
      "Epoch 43/50\n",
      "310/310 [==============================] - 63s 203ms/step - loss: 3.2658 - acc: 0.7974 - val_loss: 3.2143 - val_acc: 0.8006\n",
      "Epoch 44/50\n",
      "310/310 [==============================] - 63s 202ms/step - loss: 3.2171 - acc: 0.8004 - val_loss: 3.2170 - val_acc: 0.8004\n",
      "Epoch 45/50\n",
      "310/310 [==============================] - 62s 199ms/step - loss: 3.1424 - acc: 0.8050 - val_loss: 3.2046 - val_acc: 0.8012\n",
      "Epoch 46/50\n",
      "310/310 [==============================] - 62s 199ms/step - loss: 3.2788 - acc: 0.7966 - val_loss: 3.2133 - val_acc: 0.8006\n",
      "Epoch 47/50\n",
      "310/310 [==============================] - 63s 202ms/step - loss: 3.2334 - acc: 0.7994 - val_loss: 3.2055 - val_acc: 0.8011\n",
      "Epoch 48/50\n",
      "310/310 [==============================] - 62s 198ms/step - loss: 3.1846 - acc: 0.8024 - val_loss: 3.2170 - val_acc: 0.8004\n",
      "Epoch 49/50\n",
      "310/310 [==============================] - 69s 223ms/step - loss: 3.2204 - acc: 0.8002 - val_loss: 3.2057 - val_acc: 0.8011\n",
      "Epoch 50/50\n",
      "310/310 [==============================] - 70s 227ms/step - loss: 3.2366 - acc: 0.7992 - val_loss: 3.2198 - val_acc: 0.8002\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a2fef98d0>"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        samples_per_epoch=nb_train_samples,\n",
    "        nb_epoch=nb_epoch,\n",
    "        validation_data=validation_generator,\n",
    "        nb_val_samples=nb_validation_samples,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('basic_cnn_50_epochs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.218856064929466, 0.8002955082742317]"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_generator(validation_generator, nb_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
